---
layout: post
title:  "Generative Modelling With U-Nets"
date:   2018-10-05 20:00:00 +0200
permalink: /u_nets/
mathjax: true
---

Aim: to model/generate shape and appearance orthogonally? (bad choice of word)

Disentagnling of x and y

y estimated from the joint algorithm thing

Vanilla VAEs and why they don't work

Go over log likelihood vs probability

ELBO = difference between distribution of a latent variable and distribution of respective observed variable

?Intractable integral means no closed form and high dimensional so we can't solve numerically?

Talk about the maths behind the encoder and decoder.

Say that this: https://blog.evjang.com/2016/08/variational-bayes.html should be read first but maybe just rehash the most important bits.

Estimate y and use that estimate with the original image to estimate z. Done by maximising conditional log-likilihood ($log P(x|\hat{y})$)

The thing we are interested in for generative modelling is $P(X|Z)$ (<- is the liklihood. Z is cat label, X is image)

Talk about intractability

You sample probability dists and compute PDFs.

*Sampling* $z \sim Q(Z|X)$ is 'encoding' that converts observation $x$ to latent code $z$

*Sampling* $ x \sim Q(X|Z) $ is 'decoding' that reconstructs inputs from latents $z$

Variational lower bound is computationally tractable if you can evaluate $p(x|z), p(x), q(z|x)$ 