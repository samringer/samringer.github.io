<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-11-03T18:11:59+00:00</updated><id>http://localhost:4000/</id><title type="html">Sam Ringer</title><subtitle></subtitle><entry><title type="html">Training GANs Using Wasserstein Distance</title><link href="http://localhost:4000/wasserstein_distance/" rel="alternate" type="text/html" title="Training GANs Using Wasserstein Distance" /><published>2018-08-03T19:00:00+01:00</published><updated>2018-08-03T19:00:00+01:00</updated><id>http://localhost:4000/Wasserstein_Distance</id><content type="html" xml:base="http://localhost:4000/wasserstein_distance/">&lt;h3 id=&quot;generative-modelling&quot;&gt;Generative Modelling&lt;/h3&gt;

&lt;p&gt;Over the years, us humans have become quite good at quickly analysing data. With any luck, you and I should easily be able to tell apart a picture of a cat from a picture of a dog. This is something so easy that it takes us only milliseconds to do and even young children can do so. Untill recently, getting computers to do the same has been near impossible. It is only in the last decade that any tangible progress has been been made. Nonetheless, for simple image recognition tasks, computers have now reached a stage where they are at a human-ish level.&lt;/p&gt;

&lt;p&gt;Now let’s think about a different problem. I want you to draw for me a dog. Not a rough cartoon of a dog, but an actual photorealistic drawing. Not so easy, right? Looking at how long it took to get computers to be able to recognise cats from dogs, it’s easy to think that getting a computer to create an image of a dog that is as realistic as the one in the image below would be many decades away. You would be wrong.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/Dogs.png&quot; alt=&quot;Dogs&quot; /&gt;&lt;/p&gt;

&lt;html&gt;&lt;center&gt;&lt;i&gt;Spot The Difference&lt;/i&gt;&lt;/center&gt;&lt;/html&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Astonishingly, the dog on the right has never existed in any shape or form. It was generated in &lt;a href=&quot;https://arxiv.org/abs/1809.11096&quot;&gt;this&lt;/a&gt; paper.&lt;/p&gt;

&lt;p&gt;The field of trying to get computers to generate data that is indistinguishable from real data (whether that be pictures, audio, text &lt;em&gt;etc.&lt;/em&gt;) is known as &lt;em&gt;generative modelling&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;gans--kl-divergence&quot;&gt;GANs &amp;amp; KL Divergence&lt;/h3&gt;

&lt;p&gt;A big breakthrough in generative modelling came with the introduction of the &lt;em&gt;Generative Adversarial Network&lt;/em&gt;, or &lt;em&gt;GAN&lt;/em&gt; for short. A GAN is a specific architecture of neural network that learns to produce fake data by mimic the underlying &lt;em&gt;probability distribution&lt;/em&gt; of some real data.&lt;/p&gt;

&lt;p&gt;As the GAN aims to copy a real probability distribution, we need a loss function that can tell us how different two probability distributions are. In this case, we want the generated probability distribution to be as close to the real data probability distribution as possible, so we want to minimise said loss function.&lt;/p&gt;

&lt;p&gt;When dealing with probability distributions, it is common to use &lt;em&gt;probability density functions&lt;/em&gt;, or &lt;em&gt;PDFs&lt;/em&gt;, which are a function that return how likely a given input is.&lt;/p&gt;

&lt;p&gt;Compairing the similarity of two PDFs isn’t straightforward. The simplest way is to use a metric called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Kullback-Leibler divergence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/KL_Divergence.jpg&quot; alt=&quot;KL_Divergence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;KL divergence quantifies the difference in entropy between two distributions, but it is flawed. Note that the KL divergence between $P(x)$ and $Q(x)$ is different from the KL divergence between $Q(x)$ and $P(x)$. This is like saying the distance from London to Manchester is different from the distance from Manchester to London.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: To all intents and purposes, when we say the &lt;u&gt;distance&lt;/u&gt; between two probability distribution, we mean the dissimilarity between them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The fault is known as the &lt;em&gt;asymmetry&lt;/em&gt; of KL divergence. There are other issues with KL divergence. Most notably, when our generated probability distribution is undefined at a point (&lt;em&gt;i.e&lt;/em&gt; $Q(x) = 0$) then the KL divergence shoots off to infinity.&lt;/p&gt;

&lt;h3 id=&quot;jensen-shannon-divergence&quot;&gt;Jensen-Shannon Divergence&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot;&gt;Jensen-Shannon divergence&lt;/a&gt; aims to address this asymmetry. Like KL divergence, it is a measure of similarity between two probability distribution. However, unlike KL divergence, you get the same result by swapping $P(x)$ and $Q(x)$. The JS divergence is therefore &lt;em&gt;symmetric&lt;/em&gt;. Most traditional GANs aim to minimise a loss function that is equivalent to the JS divergence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/JS_Divergence_Transparent.png&quot; alt=&quot;JS_Divergence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sadly, the JS divergence is not the final stop on our journey to find a suitable loss function for our GAN. The nature of how probability distributions like $P(x)$ and $Q(x)$ manifest in high dimensions means that there will be many scenarios during training where either $P(x)$ or $Q(x)$ is 0 (we will explore this is more detail later). In these cases, the JS divergence experiences a sharp change and becomes non-differentiable. What is really needed is a loss function that is symmetric and smooth for all possible $P(x)$ and $Q(x)$. The Wasserstein distance is such a loss function.&lt;/p&gt;

&lt;h3 id=&quot;wasserstein-distance&quot;&gt;Wasserstein Distance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/Wasserstein_Distance.jpg&quot; alt=&quot;Wasserstein_Distance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Wasserstein distance is symmetric, but to understand its main advantage we have to take a closer look at how both the real and generated data are distrubted in a multidimensional space. (Apologies if the following gets quite abstract/dense…)&lt;/p&gt;

&lt;p&gt;Consider the following analogy. The universe we live in is a rather big 3-dimensional space. Humans, however, are only found in a very small part of that space, &lt;em&gt;i.e Earth&lt;/em&gt;. Not only that but we are only found on the &lt;em&gt;surface&lt;/em&gt; of the Earth, which appears as 2-dimensional to us humans. So humans only inhabit a small 2-dimensional space inside a much larger 3-dimensional space. In more mathematical terms, you can say that humans live on a &lt;em&gt;2-manifold&lt;/em&gt; inside a 3-dimensional space.&lt;/p&gt;

&lt;p&gt;Now, in a slight change of direction, think about a stack of 128x128 pictures of Academy Award nominee Stanley Tucci.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/Tucc_Collage.png&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;html&gt;&lt;center&gt;&lt;i&gt;Tucc&lt;/i&gt;&lt;/center&gt;&lt;/html&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Each picture of Tucci has 128x128 = 16,384 individual pixels. (Let’s keep things simply and ignore RGB). By abstracting a bit, try and imagine a 16,384 dimensional space. Each point in the space core correspondse to a different 128x128 picture. Moving along a single dimension in this space is equivalent to one pixel in our image changing value. Moreover, &lt;em&gt;every&lt;/em&gt; possible 128x128 picture you can think of correspondse to a different point in this space, whether it be a cloud, a Piña Colada, or just some random noise.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/Random_Pics.png&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;html&gt;&lt;center&gt;&lt;i&gt;Exploring The Multi-Dimensional Space&lt;/i&gt;&lt;/center&gt;&lt;/html&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Aside: It’s worth noting that nearly all points in our multi-dimensional space are going to correspond to an image that look something like the noise on the right. Points that have images with any structure or meaning will be very, very, very, very rare.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our 16,384 dimensional space, our real pictures of Tucci are going to only inhabit a very small space, most likely existsing on lower dimensional manifolds. This because there are far fewer 128x128 pictures of Tucci than there are possible 128x128 pictures.&lt;/p&gt;

&lt;p&gt;If we are training a generator to produce pictures of Stanley then the generator’s output images are likely to exist on a seperate set of manifolds in the 16,384 dimensional space. It is very, very unlikely that any of the real data’s manifolds and the generator’s manifolds are going to overlap in the multi-dimensional space. If this is the case then the probability distributions of the real and generated images are said to be &lt;em&gt;disjoint.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now we can finally get to why the Wasserstein distance is a better loss function for training GANs than the KL or JS divergences. In the likely situation where $P(x)$ and $Q(x)$ are disjoint, the KL and JS divergences often produce bad gradients, making training both the discriminator and generator very difficult. However, when we use the Wasserstein distance as a loss function, we get good gradients &lt;em&gt;everywhere&lt;/em&gt;, even if the two probability distributions are disjoint.&lt;/p&gt;

&lt;h3 id=&quot;learning-the-wasserstein-distance&quot;&gt;Learning The Wasserstein Distance&lt;/h3&gt;

&lt;p&gt;The above equation showing the Wasserstein distance is quite complicated. Annoyingly, it can’t be hand-written into your code like a more simple loss function like &lt;em&gt;MSE&lt;/em&gt;. If coding the loss function isn’t possible then how can we possibly use it? This being machine learning, the answer should be obvious. We will use a neural network to learn the Wasserstein distance for itself!&lt;/p&gt;

&lt;p&gt;This learning is possible using the &lt;em&gt;Kantorovich-Rubinstein duality&lt;/em&gt;, which is a result that expresses the Wasserstein distance in another form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/WGAN/Kantrovich_Rubenstein.jpg&quot; alt=&quot;Kantrovich_Rubenstein&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Take $f(x)$ to be the discriminator function (the discriminator takes in input $x$ and has an output $f(x)$). Consider the difference between what the discriminator outputs when it sees some real data and what it outputs when it sees some fake data. The Kantorovich-Rubenstein duality tells us that the upperbound of this difference is equal to the Wasserstein distance. This is a fairly remarkable result. However, it comes with a catch.&lt;/p&gt;

&lt;h3 id=&quot;lipschitz-functions&quot;&gt;Lipschitz Functions&lt;/h3&gt;

&lt;p&gt;The catch is that the function our discriminator learns must belong to a specific family of functions, known as &lt;em&gt;1-Lipschitz functions&lt;/em&gt;. Although it sounds complicated, Lipschitz functions can be understood on a fairly intuitive level. If we have two points, $x_1$ and $x_2$,  we can run them both through a function to get two new points, $y_1$ and $y_2$. If, for &lt;em&gt;any&lt;/em&gt; $x_1$ and $x_2$ we select, the distance between $y_1$ and $y_2$ is less than or equal to the distance between $x_1$ and $x_2$ then our function is 1-Lipschitz.&lt;/p&gt;

&lt;p&gt;The last piece in the jigsaw is making sure the function the discriminator learns is 1-Lipschitz. This is still an ongoing area of research. So far the different approaches taken have been fairly hacky. They include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clipping the weights of all the weight matricies inside the discriminator so they stay within a fixed value. This is what was done in the &lt;a href=&quot;https://arxiv.org/abs/1701.07875&quot;&gt;original WGAN paper&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Adding an extra term to our learnt Wasserstein distance loss function. This term penalises the discriminator if the norm of the gradients differ from 1. The resulting architecture is called &lt;a href=&quot;https://arxiv.org/abs/1704.00028&quot;&gt;WGAN-GP&lt;/a&gt;, the GP standing for &lt;em&gt;Gradient-Penalty&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05957&quot;&gt;Spectral normalisation&lt;/a&gt;. This idea is more theoretically sound than the methods above. Each weight matrix of the discriminator is normalised in proportion to its largest eigenvalue. Doing so limits the ‘stretchiness’ of each matrix and ensures they are 1-Lipschitz.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With any luck, you should now have enough knowledge of the Wasserstein distance to cringe everytime you read a GAN paper where an outdated loss function is used!&lt;/p&gt;</content><author><name></name></author><summary type="html">Generative Modelling</summary></entry><entry><title type="html">Deep Dive: Loss Functions</title><link href="http://localhost:4000/loss-functions/" rel="alternate" type="text/html" title="Deep Dive: Loss Functions" /><published>2018-08-03T19:00:00+01:00</published><updated>2018-08-03T19:00:00+01:00</updated><id>http://localhost:4000/loss-functions</id><content type="html" xml:base="http://localhost:4000/loss-functions/">&lt;style&gt;
.center_1 {
    display: block;
    margin-left: auto;
    margin-right: 165px;
}
.center_2 {
    display: block;
    margin-left: auto;
    margin-right: 111px;
}
.center_3 {
    display: block;
    margin-left: auto;
    margin-right: 142px;
}
.center_4 {
    display: block;
    margin-left: auto;
    margin-right: 90px;
}
.center_5 {
    display: block;
    margin-left: auto;
    margin-right: 70px;
}
.center_6 {
    display: block;
    margin-left: auto;
    margin-right: 54px;
}
&lt;/style&gt;

&lt;p&gt;When building a new deep learning model, there are four fundamental things that must be chosen:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: What will the model be trained on?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: What is the underlying structure of the model?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Loss Function&lt;/strong&gt;: How can we evaluate how well the model is doing?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: How should we make changes to the model to make it better?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-is-a-loss-function&quot;&gt;What is a loss function?&lt;/h2&gt;

&lt;p&gt;Let’s think about a simple classifier. We have a big pile of pictures that are either a picture of a panda or an armadillo and we want our network to be able to sort them into two piles:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Pandas_Armadillos.png&quot; alt=&quot;Pandas_Armadillos&quot; /&gt;&lt;/p&gt;

&lt;html&gt;&lt;center&gt;&lt;i&gt;Pandas &amp;amp; Armadillos&lt;/i&gt;&lt;/center&gt;&lt;/html&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s say we show the neural network 100 of these pictures and it makes predictions about the content of each one. We want a way of knowing how good these predictions are.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idea 1: Counting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The easiest way of assesing performance it to simple count how many correct predictions the network made. For example: “Of the 100 pictures, our network correctly classified 62 of them.”&lt;/p&gt;

&lt;p&gt;We add a bit of detail by also counting the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;What percentage of pictures classified as pandas were actually pandas?&lt;/em&gt; (True positives)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;What percentage of pictures classified as pandas were actually armadillos?&lt;/em&gt; (False positives)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;What percentage of pictures classified as armadillos were actually armadillos?&lt;/em&gt; (True negatives)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;What percentage of pictures classified as armadillos were actually pandas?&lt;/em&gt; (False negatives)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This allows us to use a method of scoring called &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision &amp;amp; recall&lt;/a&gt;. The precision is the ratio of &lt;em&gt;true positives&lt;/em&gt; to &lt;em&gt;total predictions&lt;/em&gt; and the recall is the ratio of &lt;em&gt;true positives&lt;/em&gt; to &lt;em&gt;total positives&lt;/em&gt;. The harmonic mean of the precision and recall is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F score&lt;/a&gt;. A higher F score means a more accurate model.&lt;/p&gt;

&lt;p&gt;The precision and recall technique has one big problem in the context of deep learning. It is &lt;em&gt;non-differntiable&lt;/em&gt;. This means that, although precision and recall can tell us how good the predictions are at the moment, they can’t be used to train the network to produce better predictions in the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idea 2: Change To Classifier Confidence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can do slightly better by having the network output how confident it is for its predictions for each individual image. For example, the network might say the following:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“I am 88% sure that image 6 is a panda”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;“&lt;em&gt;I am 3% sure that image 2 is a panda&lt;/em&gt;”&lt;/p&gt;

&lt;p&gt;and in terms of training, we may respond:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Well done! Image 6 was a panda, but update your parameters so next time you are closer to 100% sure, rather than 88%.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;or in the case of the second example:&lt;/p&gt;

&lt;p&gt;“&lt;em&gt;Well done! Image 2 was an armadillo. Next time try and aim for a 0% confidence instead of 3%&lt;/em&gt;”&lt;/p&gt;

&lt;p&gt;(If the second example doesn’t make sense remember that an output of “&lt;em&gt;0% panda&lt;/em&gt;” is equivalent to “&lt;em&gt;100% armadillo&lt;/em&gt;”.)&lt;/p&gt;

&lt;p&gt;This is the first example we have come across of a &lt;strong&gt;loss function&lt;/strong&gt;. A loss function lets us combine two numbers (the models prediction and the actual label) into &lt;strong&gt;one number.&lt;/strong&gt; The simple loss function above finds the difference between the prediction and the label (&lt;em&gt;100% - 88% = 12%&lt;/em&gt; for example 1 and &lt;em&gt;3% - 0% = 3%&lt;/em&gt; for example 2. In practice these are outputted as decimals &lt;em&gt;0.12&lt;/em&gt; and &lt;em&gt;0.03&lt;/em&gt;.)&lt;/p&gt;

&lt;p&gt;The errors calculated by the loss function are known as the &lt;strong&gt;loss&lt;/strong&gt;. We want to minimise the error and so a loss closer to 0 is better.&lt;/p&gt;

&lt;p&gt;Unlike precision &amp;amp; recall, loss functions are &lt;em&gt;differentiable&lt;/em&gt; and so our model can be trained! (As precision &amp;amp; recall is non-differentiable, it is called a &lt;em&gt;metric&lt;/em&gt; and not a loss function).&lt;/p&gt;

&lt;p&gt;By combining all of this, we can now understand why loss functions are so useful. They are &lt;strong&gt;differentiable&lt;/strong&gt; functions that produce &lt;strong&gt;one&lt;/strong&gt; number describing how accurate our current model is.&lt;/p&gt;

&lt;h2 id=&quot;loss-functions-in-action&quot;&gt;Loss functions in action&lt;/h2&gt;

&lt;h3 id=&quot;mean-absolute-error&quot;&gt;Mean Absolute Error&lt;/h3&gt;

&lt;p&gt;The loss function in the example above considers the raw difference between the model prediction and true label (&lt;em&gt;100% - 88% = 12%&lt;/em&gt; for example 1 and &lt;em&gt;3% - 0% = 3%&lt;/em&gt; for example 2.) This is called &lt;em&gt;absolute error&lt;/em&gt;. We  often want to combine the accuracies of many of our model’s predictions at once. One way of doing so is taking the mean. For our example, this would be $ (12\% + 3\%) \div 2 = 7.5\%$.&lt;/p&gt;

&lt;p&gt;Unsuprisingly, taking the mean of a series of absolute errors is known as &lt;strong&gt;mean absolute error&lt;/strong&gt; and is written mathematically like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Mean_Absolute_Error.png&quot; alt=&quot;Mean_Absolute_Error&quot; width=&quot;455&quot; height=&quot;75&quot; class=&quot;center_1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearing this up with some annotation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Mean_Absolute_Error_Actual.png&quot; alt=&quot;Mean_Absolute_Error_Annotated&quot; width=&quot;510&quot; height=&quot;240&quot; class=&quot;center_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(The eagle-eyed with some calculus understanding may spot that mean absolute error is not differentiable when the error is 0. Fear not as this can be side-stepped through reparameterization.)&lt;/p&gt;

&lt;p&gt;One issue with mean absolute error is that all errors are treated ‘equally’. Often we will want to penalise larger errors significantly more than small ones. The &lt;strong&gt;mean squared error&lt;/strong&gt; loss function lets us do so.&lt;/p&gt;

&lt;h3 id=&quot;mean-squared-error&quot;&gt;Mean Squared Error&lt;/h3&gt;

&lt;p&gt;By changing the absolute difference in mean absolute error to a squared difference, we can easily write down the loss function for mean squared error.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Mean_Squared_Error.png&quot; alt=&quot;Mean_Squared_Error&quot; width=&quot;465&quot; height=&quot;75&quot; class=&quot;center_3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, adding in some annotation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Mean_Squared_Error_Larger.png&quot; alt=&quot;Mean_Squared_Error_Annotated&quot; width=&quot;560&quot; height=&quot;215&quot; class=&quot;center_6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The squared term means that larger differences between $\hat{y}_i$ and $y_i$ will contribute far more to the final value of the loss function than smaller differences. Mean squared error is also directly differentiable so we don’t have to perform any reparameterization.&lt;/p&gt;

&lt;h3 id=&quot;classification--cross-entropy&quot;&gt;Classification &amp;amp; Cross-Entropy&lt;/h3&gt;

&lt;p&gt;The mean squared and mean absolute error loss function are most suited to a type of prediction known as &lt;em&gt;regression&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;When ever we are using our network to predict a continous value (&lt;em&gt;like the price of a house&lt;/em&gt; or &lt;em&gt;a person’s height&lt;/em&gt;) we are performing regression. This is opposed to &lt;em&gt;classification&lt;/em&gt; where we try to predict the &lt;em&gt;class&lt;/em&gt; of something (&lt;em&gt;e.g. What breed of dog is in this picture? Is this picture a panda or an armadillo?&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;As discussed in the above section about probabilities, the output of a classfier will be a number between 0 and 1. When performing classification, the most common loss function used is &lt;strong&gt;cross-entropy&lt;/strong&gt;. For the &lt;em&gt;binary classification&lt;/em&gt; panda-armadillo problem, it looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Cross_Entropy.png&quot; alt=&quot;Cross_Entropy&quot; width=&quot;575&quot; height=&quot;70&quot; class=&quot;center_4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the context of our example where $y_i=1$ is a picture of a panda and $y_i =0$ is a picture of an armadillo, we can add the following annotations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/images/Loss_Functions/Cross_Entropy_Larger.png&quot; alt=&quot;Cross_Entropy&quot; width=&quot;600&quot; height=&quot;292&quot; class=&quot;center_5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cross-entropy is used over the other loss functions mentioned above to improve training speed. If a classifier is correctly classifying images it will output extreme class probabilities, like &lt;em&gt;0.95&lt;/em&gt; or &lt;em&gt;0.03&lt;/em&gt;. When these extreme probabilities appear the training of your network will grind to a halt if you are using a loss function like mean squared error. The benefit of cross-entropy in classification is that it allows the model to keep learning at a decent rate, even when it is outputing extreme probabilities. (The deeper reason for this involves the magnitudes of gradients produced at extreme probabilities as a result of using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;sigmoid function&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Again, the loss functions gives us &lt;strong&gt;one&lt;/strong&gt; number that represents how accurate our network is.&lt;/p&gt;

&lt;h2 id=&quot;further-extenstions&quot;&gt;Further Extenstions&lt;/h2&gt;

&lt;p&gt;The above is very small peak into the loss function zoo. There are many simple extensions of the loss functions presented above, such as &lt;em&gt;mean absolute percentage error, hinge loss&lt;/em&gt; and &lt;em&gt;logistic loss&lt;/em&gt;.  Things can also get far more complex.&lt;/p&gt;

&lt;p&gt;For example, in a &lt;em&gt;Generative Adversarial Network (GAN)&lt;/em&gt; two neural-networks are actively fighting against each other. The loss function for the first neural network produces a better value not only when the first network performs better, but also &lt;em&gt;when the second network performs worse&lt;/em&gt; (and &lt;em&gt;vice versa&lt;/em&gt;). To obtain one coherent loss function for the whole system, the two individual loss functions must be combined into a mini-max problem.  Furthermore, recent research has improved GANs by having them learn their own loss function!&lt;/p&gt;

&lt;p&gt;It is likely that as machine learning architectures become more complex, the loss functions used will do the same. However, as the above has shown, the core question that all current loss functions address is the same: &lt;strong&gt;what function can we use to obtain one number that represents how accurate our network is?&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>