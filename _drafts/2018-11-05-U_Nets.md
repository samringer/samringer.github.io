---
layout: post
title:  "Generative Modelling With U-Nets"
date:   2018-10-05 20:00:00 +0200
permalink: /u_nets/
mathjax: true
---

https://jaan.io/what-is-variational-autoencoder-vae-tutorial/

Prerequists: variational inference, ELBOs , VAEs

This post is a breakdown of [*A Variational U-Net for Conditional Appearance and Shape Generation.*](https://arxiv.org/pdf/1804.04694.pdf)

### Aims

It's no secret that generative modelling has come a long way in recent years: the best GANs can generate photorealistic images for [every ImageNet category](https://arxiv.org/pdf/1610.09585.pdf). However, most generative models aim to generate an image directly. This means it has not been possible to control image characteristic like shape and appearance separately. When generating images of people, *shape* refers to the geometrical layout that the person is in *(sitting, running etc.)* whilst *appearance* refers to the colour and texture present in the image *(red trousers, blond hair etc.)*. Controlling shape and appearance independently has so far not been possible in generative modelling.

For instance, it has not been possible to generate and image conditioned on "*Generate an image of a man kicking a football (**shape**) wearing a pinstripe suit (**appearance**)*". The overall aim of this paper is to take *Picture A* and *Picture B* and produce a third picture, *Picture C*, that has *Picture A*'s shape and *Picture B*'s appearance.

INSERT SOME IMAGES HERE

For a deeper analysis, we need to write our above goal with some mathematical notation. First of all, let's define some variables:

$x$ - The variable representing our generated image

$y$ - The latent variable representing the desired shape of our generated image

$z$ - The latent variable representing the desired appearance of our generated image

Now we have our basic variables, the overall goal of the paper is to learn a probability distribution: $p(x|y,z)$

More verbosely, the probability distribution $p(x|y,z)$ says that, for any given appearance and shape, how likely is a given image $x$. An image that shows similar shape and appearance to those encoded by $y$ and $z$ will have a high probability whereas a random image will likely have a very low probability. 

If we learn $p(x|y,z)$ well then we can use it directly as an image generator. If we have a given $y$ and $z$ then the $x$ that maximises the probability distribution will be a generated image that has both the desired shape and appearance that we are after! Mathmatically, our best generated image is: $\text{arg max}_x p(x|y,z)$

### Attempt 1: Use A Variational Autoencoder

A key goal of this paper is to be able to combine the appearance and shape of two seperate images to generate a third. A necessary step is being able to extract the latent variables $y$ and $z$ from any input image so that we can reuse them for image generation. A common method for extracting a latent representation of an image is using a *Variational Autoencode (VAE)*.

VAE IMAGE HERE

A VAE is a neural network that learns to reconstruct its input after having gone though a low-dimensional bottleneck. If the VAE can reconstuct its input well then it is likely that the activations found at the bottleneck are a compressed representation of the input image. In other words, the bottleneck forces the VAE to *encode* the input image in an efficient low dimensional representation.

Consider a VAE that is encoding an image into a set of latents, $L$. The ELBO we try and maximise is then:

$\mathbb{E}_q \text{log} \frac{p(x|L)p(L)}{q(L|x)}$ (maybe put this is a large image an annotate?)

We evaluate $p(x|L)$ by sampling from the decoder, $q(L|x)$ by sampling from the encoder and $p(L)$ is just out prior (normally a normal distritbution).

However, our goal here is to generate images from two seperate sets of latents, $y$ and $z$. Our ELBO is then 

$\mathbb{E}_q \log\frac{p(x|y,z)p(y,z)}{q(y,z|x)}$ (Maybe rewrite in terms of logs to make things a bit bigger?)

This ELBO is still tractable and we can still sample from all the seperate terms but there is an issue with our prior $p(y,z)$. 

It is common in variational inference to use normally distributed priors. This would work completely fine for the appearance, $z$, but is not suitable for describing the spatial information encoded in $y$. This localized information is highly likely to get lost in the VAE bottleneck.

Moreover, the $p(y,z)$ prior does not guarantee the complete separation of $y$ and $z$ in the latent space. Modelling the shape and appearance of an image seperately was our key goal!

### Attempt 2: Use A *Conditional* Variational Autoencoder

Currently, we are trying to find an architecture that does two things at once: extract the shape and extract the appearance of an image.  Could we simplify things greatly by concentrating on a network that does only one of these tasks whilst 'outsourcing' the other?

There is a whole subfield of computer machine learning called *pose estimation*. Pose estimators take an input image of a human body and use a variety of algorithms to output an estimate of the locations of the bodies main joints (*including knees, hands and, of course, ELBOws*). This is incredibly similar to the task of extracting $y$ from input image $x$. This means we can bootstrap out architecture using a preexisting pose estimation model!

Using one of these models, we can take an input image, $x$, and obtain an estimate of its shape information $\hat{y} = e(x)$. Now, as we already have $x$ and an estimate for $y$, we can rewrite out ELBO to maximise:

$\mathbb{E}_q \text{log} \frac{p(x|\hat{y},z) p(z|\hat{y})}{q(z|x,\hat{y})}$ 

Our prior, $p(z|\hat{y})$, is now conditional (hence $conditional$ VAE) and can be estimated from the training data. 

(*Aside: A nice side-effect is that any relationship between $y$ and $z$ will be captured in this prior. The example used by the paper is that a person doing a star-jump is more likely to be wearing a T-shirt than black-tie.*)

### Modelling The Distributions

Through a very small amount of rearranging, we can write the above ELBO as a loss function:

ANNOTATED EQUATION 3

**Sub-networks**

Taking a step back, let's consider what consituent parts we might need to generate an image with the shape and appearance obtained from two seperate images. Firstly, we need a sub-network that can encode the shape information into the latent space. Secondly, we need a different sub-network which encodes the appearance of an image into the latent space. Finally, and most obviously, we need a generator to create an image from the shape and appearance information in the latent space.

This is exactly what the paper proposes; three different sub-networks, each with a different role. There are:

$F _\phi$ - The appearance encoder

$E _\theta$ - The shape encoder

$D _\theta$ - The image generator/decoder.

**Understanding The ELBO**

IS THE PRIOR $P(z | \hat y)$ JUST ESTIMATED BY $E_\theta$ ????

The second term represents how accurately our network can reconstruct the image. 



TALK ABOUT HOW APPEARANCE FITS INTO BOTH OF THESE TERMS

The first term can be interpretted as a form of regularisation; we want our appearance encoder $q(z|x,\hat y)$ to encode the latents as a distribution that is similar to our learnt prior $p(z| \hat y)$. The first term in the loss function therefore penalises our network according to how different the two distributions, $q(z| x, \hat y)$ and $p(z|\hat y)$, are.

We now need a plan for modelling each element of the our ELBO. We will model $q(z|x, \hat y)$ as a normal distribution. The paper now introduces a neural network, $F_\phi$, which will model the parameters (*i.e. mean and standard deviation*) of said normal distribution. This is our *appearance encoder*.

Similarly, $q(z|x, \hat y)$ is modelled as a [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution). Another neural network, $G_\theta$, to model the parameters of this Laplace distribution. This is our *image generator*.

**Modifying The Reconstruction Term**

NEED TO FILL IN ONCE THIS MAKES SENSE TO YOU

(Also explain why the VGG loss is better than the pixelwise loss)

### U-Nets

We are going to input the shape information into our network using an image similar to the one below. This is the form for our shape estimate $\hat y$.

INSERT IMAGE HERE

This $\hat y$ is going to be of the exact same size as our desired output image. If we were using a standard deep feedforward network like a vanilla VAE then the shape information contained in $\hat y$ will likely be distorted slightly as the information moves though each layer. Ideally, we would like each element of our generator to 'have access' to an undistorted (or at least less distorted) representation of $\hat y$. Using a *U-Net* architecture is a way of doing so.

ANNOTATED UNET IMAGE (use someone else simple once and draw on top)

(Maybe one not annotated and one annotated?)

(Caption: *It is still a mystery why it is called a U-Net*)



ONCE YOU HAVE THE PARAMETERS LEARNT BY F AND G IT IS DEAD EASY TO SAMPLE USING A ONE LINER

(IN THIS CASE THE BASE IS A STD NORMAL AND A STD LAPLACE)

We want to model the above ELBO using two neural networks: a generator $G_\theta$ and a discriminator $F_\phi$.

Our objective is to maximise the ELBO, but typically in mac 

**Generator**

In our final ELBO, $p(x|\hat{y},z)$ correspondse to the image generator.



GENERATOR CONSISTS OF AN ENCODER AND DECODER

The neural networks estimate parameters of the distributions! Not approximate the distributions themselves!



Standard ELBO: $\mathbb{E} [\text{log}p(x|z) + \text{log}\frac{p(z)}{q_\theta (z|x)}]$



$F_\phi$ is the encoder that extracts $z$ from x and $\hat y$ 

Penalised if z contains any shape info. It should just contain appearance info.



Aim: to model/generate shape and appearance orthogonally? (bad choice of word)

Disentagnling of x and y

y estimated from the joint algorithm thing

Vanilla VAEs and why they don't work

Go over log likelihood vs probability

ELBO = difference between distribution of a latent variable and distribution of respective observed variable

?Intractable integral means no closed form and high dimensional so we can't solve numerically?

Talk about the maths behind the encoder and decoder.

Say that this: https://blog.evjang.com/2016/08/variational-bayes.html should be read first but maybe just rehash the most important bits.

Estimate y and use that estimate with the original image to estimate z. Done by maximising conditional log-likilihood ($log P(x|\hat{y})$)

The thing we are interested in for generative modelling is $P(X|Z)$ (<- is the liklihood. Z is cat label, X is image)

Talk about intractability

You sample probability dists and compute PDFs.

*Sampling* $z \sim Q(Z|X)$ is 'encoding' that converts observation $x$ to latent code $z$

*Sampling* $ x \sim Q(X|Z) $ is 'decoding' that reconstructs inputs from latents $z$

Variational lower bound is computationally tractable if you can evaluate $p(x|z), p(x), q(z|x)$ 

OVERALL GOAL OF PAPER IS TO SEPARATELY ALTER SHAPE AND APPEARANCE (as VAE can't disentangle y and z it would fail to meet the goal of the paper).

Conditional VAE: aims to infer z from (image and estimate of y)

First step is to separate shape from appearance. Know image, estimate y, use these to find z.

Equation 3 is our loss function

Explain Unets



Step 1: Explain goal at high level

Step 2: Explain goal with a bit of maths

Step 3: Explain with some maths why VAE won't word

Step 4: Explain actual approach at high level

Step 5: Explain actual approach with maths