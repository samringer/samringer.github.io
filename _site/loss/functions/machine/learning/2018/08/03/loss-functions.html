<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Deep Dive: Loss Functions | Sam Ringer</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="Deep Dive: Loss Functions" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/loss/functions/machine/learning/2018/08/03/loss-functions.html" />
<meta property="og:url" content="http://localhost:4000/loss/functions/machine/learning/2018/08/03/loss-functions.html" />
<meta property="og:site_name" content="Sam Ringer" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-03T19:00:00+01:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/loss/functions/machine/learning/2018/08/03/loss-functions.html","headline":"Deep Dive: Loss Functions","dateModified":"2018-08-03T19:00:00+01:00","datePublished":"2018-08-03T19:00:00+01:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/loss/functions/machine/learning/2018/08/03/loss-functions.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sam Ringer" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Sam Ringer</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Dive: Loss Functions</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-08-03T19:00:00+01:00" itemprop="datePublished">Aug 3, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <style>
.center_1 {
    display: block;
    margin-left: auto;
    margin-right: 145px;
}
.center_2 {
    display: block;
    margin-left: auto;
    margin-right: 91px;
}
.center_3 {
    display: block;
    margin-left: auto;
    margin-right: 177px;
}
.center_4 {
    display: block;
    margin-left: auto;
    margin-right: 90px;
}
.center_5 {
    display: block;
    margin-left: auto;
    margin-right: 90px;
}
.center_6 {
    display: block;
    margin-left: auto;
    margin-right: 89px;
}
</style>

<p>When building a new deep learning model, there are four fundamental things that must be chosen:</p>

<ol>
  <li><strong>Data</strong>: What will the model be trained on?</li>
  <li><strong>Architecture</strong>: What is the underlying structure of the model?</li>
  <li><strong>Loss Function</strong>: How well is the model doing?</li>
  <li><strong>Optimizer</strong>: What changes should we make to the model to make it better?</li>
</ol>

<h2 id="what-is-a-loss-function">What is a loss function?</h2>

<p>Let’s think about a simple classifier. We have a big pile of pictures that are either a picture of a panda or an armadillo and we want our network to be able to sort them into two piles:</p>

<p><img src="/images/Pandas_Armadillos.png" alt="Pandas_Armadillos" /></p>

<html><center><i>Pandas &amp; Armadillos</i></center></html>

<p><br /></p>

<p>Let’s say we show the neural network 100 of these pictures and it makes predictions about the content of each one. We want a way of knowing how good these predictions are.</p>

<p><strong>Idea 1: Counting</strong></p>

<p>The easiest way of assesing performance it to simple count how many correct predictions the network made. For example “Of the 100 pictures, our network correctly classified 62 of them.” This way of scoring is called <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision &amp; recall</a>.</p>

<p>Precision and recall has one big problem in the context of deep learning. It is <em>non-differntiable</em>. This means that, although precision and recall can tell us how good the predictions are at the moment, they can’t be used to train the network to produce better predictions in the future.</p>

<p><strong>Idea 2: Change To Probability</strong></p>

<p>We can do slightly better by having the network output how confident it is for its predictions for each individual image. For example, the network might say the following:</p>

<p><em>“I am 88% sure that image 6 is a panda”</em></p>

<p>or</p>

<p>“<em>I am 3% sure Image 2 is a panda</em>”</p>

<p>and in terms of training, we may respond:</p>

<p><em>“Well done! Image 6 was a panda, but update your parameters so next time you are closer to 100% sure, rather than 88%.”</em></p>

<p>or in the case of the second example:</p>

<p>“<em>Well done! Image 2 was an armadillo. Next time try and aim for a 0% confidence instead of 3%</em>”</p>

<p>(If the second example doesn’t make sense remember that an output of “<em>0% panda</em>” is equivalent to “<em>100% armadillo</em>”.)</p>

<p>This is the first example we have come across of a <strong>loss function</strong>. A loss function lets us combine two numbers (the models prediction and the actual label) into <strong>one number.</strong> The simple loss function above finds the difference between the prediction and the label (<em>100% - 88% = 12%</em> for example 1 and <em>3% - 0% = 3%</em> for example 2. In reality these are outputted as decimals <em>0.12</em> and <em>0.03</em>.)</p>

<p>The errors calculated by the loss function are known as the <strong>loss</strong>. We want to minimise the error and so a loss closer to 0 is better.</p>

<p>Unlike precision &amp; recall, loss functions are <em>differentiable</em> and so our model can be trained!</p>

<p>By combining all of this, we can now understand why loss functions are so useful. They are <strong>differentiable</strong> functions that produce <strong>one</strong> number describing how accurate our current model is.</p>

<h2 id="loss-functions-in-action">Loss functions in action</h2>

<h3 id="mean-absolute-error">Mean Absolute Error</h3>

<p>The loss function considers the raw difference between the model prediction and true label (<em>100% - 88% = 12%</em> for example 1 and <em>3% - 0% = 3%</em> for example 2.) This is called <em>absolute error</em>. It is often the case that we want to combine the accuracies of many of our model’s predictions at once. One way of doing so is taking the mean. For our example, this would be $ (12\% + 3\%) \div 2 = 7.5\%$.</p>

<p>Unsuprisingly, taking the mean of a series of absolute errors is known as <strong>mean absolute error</strong> and is written mathematically like this:</p>

<p><img src="/images/Mean_Absolute_Error.png" alt="Mean_Absolute_Error" width="455" height="75" class="center_1" /></p>

<p>Clearing this up with some annotation:</p>

<p><img src="/images/Mean_Absolute_Error_Actual.png" alt="Mean_Absolute_Error_Annotated" width="510" height="240" class="center_2" /></p>

<p>(The eagle-eyed with some calculus understanding may spot that mean absolute error is not differentiable when the error is 0. Fear not as this can be side-stepped by reparameterization.)</p>

<p>One issue with mean absolute error is that all errors are treated ‘equally’. Often we will want to penalise larger errors significantly more than small ones. The <strong>mean squared error</strong> loss function lets us do so.</p>

<h3 id="mean-squared-error">Mean Squared Error</h3>

<p>By changing the absolute difference in mean absolute error to a squared difference, we can easily write down the loss function for mean squared error.</p>

<p><img src="/images/Mean_Squared_Error.png" alt="Mean_Squared_Error" width="465" height="75" class="center_3" /></p>

<p>Again, adding in some annotation:</p>

<p><img src="/images/Mean_Squared_Error_Larger.png" alt="Mean_Squared_Error_Annotated" width="560" height="215" class="center_6" /></p>

<p>The squared term means that larger differences between $\hat{y}_i$ and $y_i$ will contribute far more to the final value of the loss function than smaller differences. Mean squared error is also directly differentiable so we don’t have to perform any reparameterization.</p>

<h3 id="classification--cross-entropy">Classification &amp; Cross-Entropy</h3>

<p>The mean squared and mean absolute error loss function are most suited to a type of prediction known as <em>regression</em>.</p>

<p>Our network can be trained to output</p>

<p>When ever we are using our network to predict a continous value (<em>like the price of a house or a person’s height</em>) we are performing regression. This is opposed to <em>classification</em> where we try to predict the <em>class</em> of something (<em>e.g. What breed of dog is in this picture? Is this picture a panda or an armadillo?</em>).</p>

<p>As discussed in the above section about probabilities, the output of a classfier will be a number between 0 and 1. In this situation, we can speed up the training of the network using a loss function called <strong>cross-entropy</strong>. For the <em>binary classification</em> panda-armadillo problem, it looks like this:</p>

<p><img src="/images/Cross_Entropy.png" alt="Cross_Entropy" width="575" height="75" class="center_4" /></p>

<p>In the context of our example where $y_i=1$ is a picture of a panda and $y_i =0$ is a picture of an armadillo, we can add the following annotations:</p>

<p><img src="/images/Cross_Entropy_Larger.png" alt="Cross_Entropy_Annotated" width="580" height="290" class="center_5" /></p>

<p>Again, the loss functions gives us <strong>one</strong> number that represents how accurate our network is.</p>

<h2 id="further-extenstions">Further Extenstions</h2>

<p>The above is very small peak into the loss function zoo. There are many simple extensions of the loss functions presented above, such as <em>mean absolute percentage error, hinge loss</em> and <em>logistic loss</em>  Things can also get far more complex. For example, there are</p>

<p>Loss function is only a function of model weights and biases.</p>

<p>Talk about the mountain scape. Changing model parameters is like walking. Optimizers are which direction and how far. Different points on landscape are different parameters.</p>

<p>Different loss functions define different landscapes.</p>

<p>Quantify how well the prediction made by the network agree with the actual labels.</p>

<h3 id="regression">Regression</h3>

<p>MSE (quite sensitive to outliers.) (When might this sensitivty to outliers be useful.)</p>

<p>Mean absolute percentage error loss (Maybe don’t include this and others that ern’t as useful) “So is the MAE. The MSLE and the MAPE are worth taking into consideration if our network is predicting outputs that vary largely in range. Suppose that a network is to predict two output variables: one in the range of [0, 10] and the other in the range of [0, 100]. In this case, the MAE and the MSE will penal‐ ize the error in the second output more significantly than the first. The MAPE makes it a relative error and therefore doesn’t discriminate based on the range. The MSLE squishes the range of all the outputs down, simply how 10 and 100 translate to 1 and 2 (in log base 10). “ This is normally dealt with with standard normalisation.</p>

<h3 id="reconstruction">Reconstruction</h3>

<p>Used for autoencoders (Maybe skim over this and go over in more depth another time.)</p>


  </div><a class="u-url" href="/loss/functions/machine/learning/2018/08/03/loss-functions.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">


    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"><li><a class="u-email" href="mailto:ringer.sam93@gmail.com">ringer.sam93@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/samringer"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">samringer</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
